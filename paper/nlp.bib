@inproceedings{GermEval2019,
  author    = {Julia Maria Stru{\ss} and Melanie Siegel and Josef Ruppenhofer and Michael Wiegand and Manfred Klenner},
  title     = {Overview of GermEval Task 2, 2019 shared task on the identification of offensive language},
  booktitle = {Preliminary proceedings of the 15th Conference on Natural Language Processing (KONVENS 2019), October 9 – 11, 2019 at Friedrich-Alexander-Universit{\"a}t Erlangen-N{\"u}rnberg},
  publisher = {German Society for Computational Linguistics \& Language Technology und Friedrich-Alexander-Universit{\"a}t Erlangen-N{\"u}rnberg},
  address   = {M{\"u}nchen [u.a.]},
  url       = {https://nbn-resolving.org/urn:nbn:de:bsz:mh39-93197},
  pages     = {352 -- 363},
  year      = {2019},
  abstract  = {We present the second edition of the GermEval Shared Task on the Identification of Offensive Language. This shared task deals with the classification of German tweets from Twitter. Two subtasks were continued from the first edition, namely a coarse-grained binary classification task and a fine-grained multi-class classification task. As a novel subtask, we introduce the classification of offensive tweets as explicit or implicit. The shared task had 13 participating groups submitting 28 runs for the coarse-grained task, another 28 runs for the fine-grained task, and 17 runs for the implicit-explicit task. We evaluate the results of the systems submitted to the shared task. The shared task homepage can be found at https://projects.fzai.h-da.de/iggsa/},
  language  = {en}
}


@incollection{GermEval2018,
  author    = {Michael Wiegand and Melanie Siegel and Josef Ruppenhofer},
  title     = {Overview of the GermEval 2018 Shared Task on the Identification of Offensive Language},
  booktitle = {Proceedings of GermEval 2018, 14th Conference on Natural Language Processing (KONVENS 2018), Vienna, Austria – September 21, 2018},
  publisher = {Austrian Academy of Sciences},
  address   = {Vienna, Austria},
  isbn      = {978-3-7001-8435-5},
  url       = {https://nbn-resolving.org/urn:nbn:de:bsz:mh39-84935},
  pages     = {1 -- 10},
  year      = {2019},
  abstract  = {We present the pilot edition of the GermEval Shared Task on the Identification of Offensive Language. This shared task deals with the classification of German tweets from Twitter. It comprises two tasks, a coarse-grained binary classification task and a fine-grained multi-class classification task. The shared task had 20 participants submitting 51 runs for the coarse-grained task and 25 runs for the fine-grained task. Since this is a pilot task, we describe the process of extracting the raw-data for the data collection and the annotation schema. We evaluate the results of the systems submitted to the shared task. The shared task homepage can be found at https://projects.cai. fbi.h-da.de/iggsa/},
  language  = {en}
}

@inproceedings{germeval2021overview,
  title     = {Overview of the {G}erm{E}val 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments},
  author    = {Risch, Julian and Stoll, Anke and Wilms, Lena and Wiegand, Michael},
  booktitle = {Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments co-located with KONVENS},
  pages     = {1 -- 12},
  year      = {2021}
}

@inproceedings{BERT,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1423},
  doi       = {10.18653/v1/N19-1423},
  pages     = {4171--4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@article{RoBERTa_old,
  abstract      = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  arxivid       = {1907.11692},
  author        = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  eprint        = {1907.11692},
  file          = {:Users/shanest/Documents/Library/Liu et al/Unknown/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining Approach.pdf:pdf},
  keywords      = {model},
  title         = {{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
  url           = {http://arxiv.org/abs/1907.11692},
  year          = {2019}
}

@article{RoBERTa,
  title   = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author  = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and M. Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal = {ArXiv},
  year    = {2019},
  url     = {http://arxiv.org/abs/1907.11692},
  volume  = {abs/1907.11692}
}

@inproceedings{ELECTRA,
  title     = {{ELECTRA}: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author    = {Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
  booktitle = {ICLR},
  year      = {2020},
  url       = {https://openreview.net/pdf?id=r1xMH1BtvB}
}

@article{GPT2,
  title   = {Language models are unsupervised multitask learners},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal = {OpenAI blog},
  url     = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
  year    = {2019}
}

@inproceedings{GPT3,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  title     = {Language Models are Few-Shot Learners},
  url       = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@inproceedings{huggingface,
  title     = {Transformers: State-of-the-Art Natural Language Processing},
  author    = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month     = oct,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
  pages     = {38--45}
}


@inproceedings{GNLM,
  title     = {{G}erman{'}s Next Language Model},
  author    = {Chan, Branden and
Schweter, Stefan and
M{\"o}ller, Timo},
  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
  year      = {2020},
  address   = {Barcelona, Spain (Online)},
  publisher = {International Committee on Computational Linguistics},
  url       = {https://aclanthology.org/2020.coling-main.598},
  doi       = {10.18653/v1/2020.coling-main.598},
  pages     = {6788--6796}
}

@inproceedings{Transformer,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Attention is All you Need},
  url       = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  volume    = {30},
  year      = {2017}
}

@article{Zeit_GPT3,
  author  = {Christoph Dr{\"{o}}sser},
  title   = {Sie klingt wie wir. {E}ine {S}oftware vermittelt die {I}llusion eines {Z}wiegespr{\"{a}}chs.},
  journal = {Die Zeit},
  volume  = {54/2020},
  year    = {2020}
}

@article{GPT3-Code,
  author  = {Cade Metz},
  title   = {Meet {GPT-3}. {I}t Has Learned to Code (and Blog and Argue)},
  journal = {New York Times},
  year    = {2020},
  date    = {2020/11/24},
  url     = {https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html}
}

@article{10.1145/3381831,
  author     = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
  title      = {Green AI},
  year       = {2020},
  issue_date = {December 2020},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {63},
  number     = {12},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/3381831},
  doi        = {10.1145/3381831},
  abstract   = {Creating efficiency in AI research will decrease its carbon footprint and increase
its inclusivity as deep learning study should not require the deepest pockets.},
  journal    = {Commun. ACM},
  month      = nov,
  pages      = {54–63},
  numpages   = {10}
}



@article{DBLP:journals/corr/abs-2007-01852,
  author        = {Fangxiaoyu Feng and
               Yinfei Yang and
               Daniel Cer and
               Naveen Arivazhagan and
               Wei Wang},
  title         = {Language-agnostic {BERT} Sentence Embedding},
  journal       = {CoRR},
  volume        = {abs/2007.01852},
  year          = {2020},
  url           = {https://arxiv.org/abs/2007.01852},
  archiveprefix = {arXiv},
  eprint        = {2007.01852},
  timestamp     = {Mon, 06 Jul 2020 15:26:01 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2007-01852.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{10.1257/jep.31.2.211,
  author  = {Allcott, Hunt and Gentzkow, Matthew},
  title   = {Social Media and Fake News in the 2016 Election},
  journal = {Journal of Economic Perspectives},
  volume  = {31},
  number  = {2},
  year    = {2017},
  month   = {May},
  pages   = {211-36},
  doi     = {10.1257/jep.31.2.211},
  url     = {https://www.aeaweb.org/articles?id=10.1257/jep.31.2.211}
}

@article{Bovet2019,
  author   = {Bovet, Alexandre
and Makse, Hern{\'a}n A.},
  title    = {Influence of fake news in Twitter during the 2016 US presidential election},
  journal  = {Nature Communications},
  year     = {2019},
  month    = {Jan},
  day      = {02},
  volume   = {10},
  number   = {1},
  pages    = {7},
  abstract = {The dynamics and influence of fake news on Twitter during the 2016 US presidential election remains to be clarified. Here, we use a dataset of 171 million tweets in the five months preceding the election day to identify 30 million tweets, from 2.2 million users, which contain a link to news outlets. Based on a classification of news outlets curated by www.opensources.co, we find that 25{\%} of these tweets spread either fake or extremely biased news. We characterize the networks of information flow to find the most influential spreaders of fake and traditional news and use causal modeling to uncover how fake news influenced the presidential election. We find that, while top influencers spreading traditional center and left leaning news largely influence the activity of Clinton supporters, this causality is reversed for the fake news: the activity of Trump supporters influences the dynamics of the top fake news spreaders.},
  issn     = {2041-1723},
  doi      = {10.1038/s41467-018-07761-2},
  url      = {https://doi.org/10.1038/s41467-018-07761-2}
}

@article{AdamW,
  author        = {Ilya Loshchilov and
               Frank Hutter},
  title         = {Fixing Weight Decay Regularization in Adam},
  journal       = {CoRR},
  volume        = {abs/1711.05101},
  year          = {2017},
  url           = {http://arxiv.org/abs/1711.05101},
  archiveprefix = {arXiv},
  eprint        = {1711.05101},
  timestamp     = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{Adafactor,
  author        = {Noam Shazeer and
               Mitchell Stern},
  title         = {Adafactor: Adaptive Learning Rates with Sublinear Memory Cost},
  journal       = {CoRR},
  volume        = {abs/1804.04235},
  year          = {2018},
  url           = {http://arxiv.org/abs/1804.04235},
  archiveprefix = {arXiv},
  eprint        = {1804.04235},
  timestamp     = {Mon, 13 Aug 2018 16:48:15 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/abs-1804-04235.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@conference{Kluyver2016jupyter,
  title        = {Jupyter Notebooks -- a publishing format for reproducible computational workflows},
  author       = {Thomas Kluyver and Benjamin Ragan-Kelley and Fernando P{\'e}rez and Brian Granger and Matthias Bussonnier and Jonathan Frederic and Kyle Kelley and Jessica Hamrick and Jason Grout and Sylvain Corlay and Paul Ivanov and Dami{\'a}n Avila and Safia Abdalla and Carol Willing},
  booktitle    = {Positioning and Power in Academic Publishing: Players, Agents and Agendas},
  editor       = {F. Loizides and B. Schmidt},
  organization = {IOS Press},
  pages        = {87--90},
  year         = {2016}
}

@incollection{Bisong2019,
  author    = {Bisong, Ekaba},
  title     = {Google Colaboratory},
  booktitle = {Building Machine Learning and Deep Learning Models on Google Cloud Platform: A Comprehensive Guide for Beginners},
  year      = {2019},
  publisher = {Apress},
  address   = {Berkeley, CA},
  pages     = {59--64},
  abstract  = {Google Colaboratory more commonly referred to as ``Google Colab'' or just simply ``Colab'' is a research project for prototyping machine learning models on powerful hardware options such as GPUs and TPUs. It provides a serverless Jupyter notebook environment for interactive development. Google Colab is free to use like other G Suite products.},
  isbn      = {978-1-4842-4470-8},
  doi       = {10.1007/978-1-4842-4470-8_7},
  url       = {https://doi.org/10.1007/978-1-4842-4470-8_7}
}

@article{ClarinD,
  author  = {Erhard Hinrichs and Thorsten Trippel},
  doi     = {doi:10.1515/bfp-2017-0015},
  url     = {https://doi.org/10.1515/bfp-2017-0015},
  title   = {{CLARIN-D: eine Forschungsinfrastruktur f{\"u}r die sprachbasierte Forschung in den Geistes- und Sozialwissenschaften}},
  journal = {Bibliothek Forschung und Praxis},
  number  = {1},
  volume  = {41},
  year    = {2017},
  pages   = {45--54}
}

@article{bpe,
  author     = {Gage, Philip},
  title      = {A New Algorithm for Data Compression},
  year       = {1994},
  issue_date = {Feb. 1994},
  publisher  = {R & D Publications, Inc.},
  address    = {USA},
  volume     = {12},
  number     = {2},
  issn       = {0898-9788},
  journal    = {C Users J.},
  month      = feb,
  pages      = {23–-38},
  numpages   = {16}
}

@inproceedings{SentencePiece,
  author    = {Taku Kudo and
               John Richardson},
  editor    = {Eduardo Blanco and
               Wei Lu},
  title     = {SentencePiece: {A} simple and language independent subword tokenizer
               and detokenizer for Neural Text Processing},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2018: System Demonstrations, Brussels,
               Belgium, October 31 - November 4, 2018},
  pages     = {66--71},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  url       = {https://doi.org/10.18653/v1/d18-2012},
  doi       = {10.18653/v1/d18-2012},
  timestamp = {Tue, 28 Jan 2020 10:28:21 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/KudoR18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{german-nlp-group/electra-base-german-uncased,
  author       = {Philipp Reissel and Philip May},
  title        = {{G}erman {E}lectra {U}ncased},
  howpublished = {Huggingface model hub},
  year         = {2020},
  url          = {https://huggingface.co/german-nlp-group/electra-base-german-uncased},
  note         = {\url{https://huggingface.co/german-nlp-group/electra-base-german-uncased}, Last accessed on 2021-07-12}
}

@misc{benjamin/gerpt2-large,
  author       = {Benjamin Minixhofer},
  title        = {{GerPT2-large} -- {A} large {G}erman {GPT2}},
  howpublished = {Huggingface model hub},
  year         = {2020},
  note         = {\url{https://huggingface.co/benjamin/gerpt2-large}, Last accessed on 2021-07-12}
}

@misc{wandb,
  title        = {Experiment Tracking with Weights and Biases},
  year         = {2020},
  howpublished = {Company website},
  note         = {Application available on \href{https://www.wandb.com/}{wandb.com}, Last accessed on 2021-07-12},
  author       = {Biewald, Lukas}
}


@misc{GC4,
  author       = {Philipp Reissel and Philip May},
  title        = {{GC4} {C}orpus},
  howpublished = {GitHub pages},
  year         = {2021},
  url          = {https://german-nlp-group.github.io/projects/gc4-corpus.html},
  note         = {\url{https://german-nlp-group.github.io/projects/gc4-corpus.html}, Last accessed on 2021-07-12}
}
